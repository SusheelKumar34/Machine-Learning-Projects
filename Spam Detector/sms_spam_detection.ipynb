{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09f79f88",
   "metadata": {},
   "source": [
    "## SMS Spam Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148e88d0",
   "metadata": {},
   "source": [
    "<p>You are hired as an AI expert in the development department of a telecommunications company. The first thing on your orientation plan is a small project that your boss has assigned you for the following given situation. Your supervisor has given away his private cell phone number on too many websites and is now complaining about daily spam SMS. Therefore, it is your job to write a spam detector in Python. </p>\n",
    "\n",
    "<p>In doing so, you need to use a Naive Bayes classifier that can handle both bag-of-words (BoW) and tf-idf features as input. For the evaluation of your spam detector, an SMS collection is available as a dataset - this has yet to be suitably split into train and test data. To keep the costs as low as possible and to avoid problems with copyrights, your boss insists on a new development with Python.</p>\n",
    "\n",
    "<p>Include a short description of the data preprocessing steps, method, experiment design, hyper-parameters, and evaluation metric. Also, document your findings, drawbacks, and potential improvements.</p>\n",
    "\n",
    "<p>Note: You need to implement the bag-of-words (BoW) and tf-idf feature extractor from scratch. You can use existing python libraries for other tasks.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad12eba",
   "metadata": {},
   "source": [
    "**Dataset and Resources**\n",
    "\n",
    "* SMS Spam Collection Dataset: https://archive.ics.uci.edu/dataset/228/sms+spam+collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8829270",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7063908e",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dba0e9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "88e9e501",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocesses the text by removing non-alphanumeric characters and converting to lowercase.\n",
    "    \"\"\"\n",
    "    text = text.replace('\\n', ' ')  # convert new line character to space\n",
    "    text = text.lower()  # Convert to lower case\n",
    "    text = text.split(' ')  # tokenisation with spaces\n",
    "    \n",
    "    # tokenisation with tabs\n",
    "    xtext = []\n",
    "    for i in range(len(text)):  \n",
    "        for ele in text[i].split('\\t'):\n",
    "            xtext.append(ele)\n",
    "    text = xtext\n",
    "    \n",
    "    # remove special characters\n",
    "    xtext = []\n",
    "    for i in text:  \n",
    "        if re.sub(r'[^a-zA-Z\\d\\s]', '', i) !='':\n",
    "            xtext.append(re.sub(r'[^a-zA-Z\\d\\s]', '', i))\n",
    "    text = xtext\n",
    "\n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62ee1f6",
   "metadata": {},
   "source": [
    "### Preprocessing \n",
    "* convert the text to lowercase\n",
    "* Removing special characters and spaces\n",
    "* tokenization\n",
    "* splitting the text, between classification and the messages\n",
    "* converting the ham/spam to binary values to make things easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "50918e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('D:\\\\BS. CS & AI\\\\Natural Language Understanding\\\\Practical Course\\\\Assignment_2\\\\Assignment_2\\\\_sms_spam_collection\\\\SMSSpamCollection') as f:\n",
    "    corpus = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8ef2e436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the corpus\n",
    "for i in range(len(corpus)):\n",
    "\tcorpus[i] = preprocess_text(corpus[i])\n",
    "corpus\n",
    "\n",
    "# remove the empty documents\n",
    "l = len(corpus) - 1\n",
    "for i in range(len(corpus)):\n",
    "\tif ''.join(corpus[l-i].split())=='':\n",
    "\t\tprint(corpus[l-i])\n",
    "\t\tdel corpus[l-i]\n",
    "\n",
    "# remove classification word and create classification_labels list\n",
    "classification_labels = []\n",
    "for i in range(len(corpus)):\n",
    "\tdocument = corpus[i]\n",
    "\n",
    "\t# extract label from document and add it to classification_labels\n",
    "\tlabel = document.split(' ')[0]\n",
    "\tclassification_labels.append(label)\n",
    "\n",
    "\t# remove label from document in the corpus\n",
    "\tcorpus[i] = ' '.join(document.split(' ')[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "131391fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert ham to 0 and spam to 1 in classification list\n",
    "classification_labels = [0 if label=='ham' else 1 for label in classification_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9ee05bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare vocabulary\n",
    "vocab = set()\n",
    "for document in corpus:\n",
    "\tfor word in document.split(' '):\n",
    "\t\tvocab.add(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f727052",
   "metadata": {},
   "source": [
    "### Bag of Words implemented in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f4109920",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(corpus, vocabulary):\n",
    "    '''\n",
    "        Returns the vector of bag of words\n",
    "    '''\n",
    "    vectors = []\n",
    "\n",
    "    # convert vocabulary to list to get the index by its element\n",
    "    vocabulary = list(vocabulary) \n",
    "\n",
    "    for document in corpus:\n",
    "        # Tokenize the document into words\n",
    "        words = document.lower().split()\n",
    "\n",
    "        # Create a dictionary to store the word counts\n",
    "        word_counts = defaultdict(int)\n",
    "\n",
    "        # Count the occurrences of each word\n",
    "        for word in words:\n",
    "            word_counts[word] += 1\n",
    "\n",
    "        # Create the bag-of-words vector\n",
    "        vector = np.zeros(len(vocabulary))\n",
    "        for word, count in word_counts.items():\n",
    "            if word in vocabulary:\n",
    "                vector[vocabulary.index(word)] = count\n",
    "        \n",
    "        vectors.append(vector)\n",
    "\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "baa3982d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vector = bag_of_words(corpus, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eaf01a9",
   "metadata": {},
   "source": [
    "### Tf-idf implemented in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b4235849",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf(word, document):     \n",
    "\t# Calculate tf\n",
    "\treturn document.count(word) / len(document)\n",
    "\n",
    "def idf(word, corpus):\n",
    "\t# Calculate idf\n",
    "    # +1 to avoid division by 0\n",
    "    count_of_documents = len(corpus) + 1\n",
    "    count_of_documents_with_word = sum([1 for doc in corpus if word in doc]) + 1\n",
    "    idf = np.log10(count_of_documents/count_of_documents_with_word) + 1\n",
    "    return idf\n",
    "\n",
    "def tf_idf(word, document, corpus):\n",
    "\t# Calculate tf-idf\n",
    "\treturn tf(word, document) * idf(word, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "91267f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index = {word:i for i, word in enumerate(vocab)}\n",
    "num_words = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "265a6f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an empty list to store our word vectors\n",
    "tf_idf_vector = []\n",
    "for document in corpus:\n",
    "    # for our new document create a new word vector\n",
    "    new_word_vector = [0 for i in range(num_words)]\n",
    "    \n",
    "    # now we loop through each word in our document and compute the tf-idf score and populate our vector with it,\n",
    "    # we only care about words in this document because words outside of it will remain zero\n",
    "    for word in document.split():\n",
    "        # get the score\n",
    "        tf_idf_score = tf_idf(word, document, corpus)\n",
    "        # next get the index for this word in our word vector\n",
    "        word_index = word_to_index[word] \n",
    "        # populate the vector\n",
    "        new_word_vector[word_index] = tf_idf_score\n",
    "    \n",
    "    # don't forget to add this new word vector to our list of existing tf_idf_vector\n",
    "    tf_idf_vector.append(new_word_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a40fd0a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5574\n",
      "9578\n"
     ]
    }
   ],
   "source": [
    "print(len(bow_vector))\n",
    "print(len(bow_vector[0]))\n",
    "# print(bow_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0cb61ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5574\n",
      "9578\n"
     ]
    }
   ],
   "source": [
    "print(len(tf_idf_vector))\n",
    "print(len(tf_idf_vector[0]))\n",
    "# print(tf_idf_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a12c8cd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5574"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(classification_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ea183365",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9578"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a059f7",
   "metadata": {},
   "source": [
    "# Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bca7bdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33b92c2",
   "metadata": {},
   "source": [
    "#### Model using Bag of words vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f846802d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting into test and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(bow_vector, classification_labels, test_size=0.2, random_state=4, stratify=classification_labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4caa49f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    3861\n",
       "1     598\n",
       "dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(y_train)\n",
    "df.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2fcdaf97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    966\n",
       "1    149\n",
       "dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(y_test)\n",
    "df.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "adba2cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "gnb = GaussianNB()\n",
    "mnb = MultinomialNB()\n",
    "bnb = BernoulliNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5ba45f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9076233183856502\n",
      "[[872  94]\n",
      " [  9 140]]\n",
      "0.5982905982905983\n"
     ]
    }
   ],
   "source": [
    "gnb.fit(X_train, y_train)\n",
    "y_pred1 = gnb.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred1))\n",
    "print(confusion_matrix(y_test, y_pred1))\n",
    "print(precision_score(y_test, y_pred1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9ebdb8",
   "metadata": {},
   "source": [
    "The Gaussian Naive Bayes classifier predicts approximately 90.76% accurately, indicating that the model performs well overall. The model has high number of true positives and true negatives but still there are many false positives and negatives, indicating some room for improvement. as we can see from the precision score, only 59.82% times it actually classifies spam correctly which is not got at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "19d013b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9829596412556054\n",
      "[[955  11]\n",
      " [  8 141]]\n",
      "0.9276315789473685\n"
     ]
    }
   ],
   "source": [
    "mnb.fit(X_train, y_train)\n",
    "y_pred2 = mnb.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred2))\n",
    "print(confusion_matrix(y_test, y_pred2))\n",
    "print(precision_score(y_test, y_pred2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6117d825",
   "metadata": {},
   "source": [
    "The Multinomial Naive Bayes classifer gives better results compared to GNB in both overall accuracy and precision score, meaning that it tackles the imbalanced dataset accurately learning both classes correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a2ae3307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.979372197309417\n",
      "[[963   3]\n",
      " [ 20 129]]\n",
      "0.9772727272727273\n"
     ]
    }
   ],
   "source": [
    "bnb.fit(X_train, y_train)\n",
    "y_pred3 = bnb.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred3))\n",
    "print(confusion_matrix(y_test, y_pred3))\n",
    "print(precision_score(y_test, y_pred3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80dd57a6",
   "metadata": {},
   "source": [
    "The Bernoulli Naive Bayes classifier also gives better results than GNB, learning the classes better and having less false positives and negatives "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d0aaf5",
   "metadata": {},
   "source": [
    "#### Model using tf-idf vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0e094a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tf_idf_vector, classification_labels, test_size=0.2, random_state=4, stratify=classification_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "906f0888",
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()\n",
    "mnb = MultinomialNB()\n",
    "bnb = BernoulliNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "70047455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9103139013452914\n",
      "[[876  90]\n",
      " [ 10 139]]\n",
      "0.6069868995633187\n"
     ]
    }
   ],
   "source": [
    "gnb.fit(X_train, y_train)\n",
    "y_pred1 = gnb.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred1))\n",
    "print(confusion_matrix(y_test, y_pred1))\n",
    "print(precision_score(y_test, y_pred1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff9d177",
   "metadata": {},
   "source": [
    "Here also Gaussian NB classifer fails to perform well on imbalanced dataset even using tf-idf vector instead of bag of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1fd4d3f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8663677130044843\n",
      "[[966   0]\n",
      " [149   0]]\n",
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python3.11.0\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "mnb.fit(X_train, y_train)\n",
    "y_pred2 = mnb.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred2))\n",
    "print(confusion_matrix(y_test, y_pred2))\n",
    "print(precision_score(y_test, y_pred2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddbc248",
   "metadata": {},
   "source": [
    "The Multinomial NB classifier fails completely here, not predicting any positive instances correctly, resulting in precision score of 0. This can be due to class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4a0eb693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.979372197309417\n",
      "[[963   3]\n",
      " [ 20 129]]\n",
      "0.9772727272727273\n"
     ]
    }
   ],
   "source": [
    "bnb.fit(X_train, y_train)\n",
    "y_pred3 = bnb.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred3))\n",
    "print(confusion_matrix(y_test, y_pred3))\n",
    "print(precision_score(y_test, y_pred3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d19af69",
   "metadata": {},
   "source": [
    "Bernoulli NB classifer also performs well on tf-idf vector just same as Bag of words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3945419c",
   "metadata": {},
   "source": [
    "Here we can conclude according to all results that the Bernoulli NB classifer turns out to be best when there is class imbalance, achieving around 98% precision and accuracy score. \n",
    "Also it doesn't matter according to our results that if we use bag of words or tf-idf vector, as results are almost same in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cbc82a",
   "metadata": {},
   "source": [
    "### Additional Experiments *(5 additional points - <span style=\"color: red;\">Optional</span>)*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
